{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fcec894",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b715631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from data import pacs\n",
    "import models.resnet_ms as resnet_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f566d",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ec42f",
   "metadata": {},
   "source": [
    "## Regarding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 7\n",
    "CLASSES = [\"dog\", \"elephant\", \"giraffe\", \"guitar\", \"horse\", \"house\", \"person\"]\n",
    "DOMAINS = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f852e",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea520f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "REGULARIZATION = 1e-2\n",
    "MODEL = resnet_ms.resnet18_fc512_ms12_a0d2\n",
    "USE_PRETRAINED = True\n",
    "OPTIMIZER = optim.AdamW\n",
    "OPTIMIZER_KWARGS = {\n",
    "    \"lr\": LEARNING_RATE, # Standard value for AdamW: 1e-3\n",
    "    \"weight_decay\": REGULARIZATION # Standard value for AdamW: 1e-2\n",
    "}\n",
    "SCHEDULER = optim.lr_scheduler.ReduceLROnPlateau\n",
    "SCHEDULER_KWARGS = {\"mode\": \"min\", \"patience\": 3}\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "EARLY_STOPPING_DELTA = 1e-5\n",
    "AUGMENTATIONS = ()\n",
    "NUM_SEEDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad6615",
   "metadata": {},
   "source": [
    "## Image Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54ab49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values for pretrained ResNet\n",
    "pretrained_image_transform = T.Compose([\n",
    "    *AUGMENTATIONS,\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc381c",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a84b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2ea28",
   "metadata": {},
   "source": [
    "## Abstract model building, optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3f6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model = lambda: MODEL(NUM_CLASSES, loss='softmax', pretrained=USE_PRETRAINED)\n",
    "build_optimizer = lambda model: OPTIMIZER(model.parameters(), **OPTIMIZER_KWARGS)\n",
    "build_scheduler = lambda optimizer: SCHEDULER(optimizer, **SCHEDULER_KWARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc97cec",
   "metadata": {},
   "source": [
    "# Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705b34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 42\n",
    "# torch.manual_seed(42)\n",
    "# if device == torch.device(\"cuda\"):\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c39917",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6119fbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb82bc",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88de7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db9d8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target, output):\n",
    "    batch_size = target.shape[0]\n",
    "    _, pred = torch.max(output, dim=-1)\n",
    "    correct = pred.eq(target).sum()\n",
    "    return correct.item() / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "572bc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch: int,\n",
    "        target_domain: str,\n",
    "        data_loader:torch.utils.data.DataLoader,\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer\n",
    "        ) -> tuple[float, float]:\n",
    "    \"\"\"train one epoch\"\"\"\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "\n",
    "    for i, (data, target) in enumerate(data_loader):\n",
    "        step = (epoch - 1) * len(data_loader) + i + 1\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        out = model(data)\n",
    "        loss = F.cross_entropy(out, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy(target, out)\n",
    "        losses.update(loss.item(), out.shape[0])\n",
    "        accs.update(acc, out.shape[0])\n",
    "\n",
    "        writer.add_scalar(f'Loss/Train/target={target_domain}', loss.item(), step)\n",
    "        writer.add_scalar(f'Accuracy/Train/target={target_domain}', acc, step)\n",
    "\n",
    "    return losses.avg, accs.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173b90b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfd3c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader: torch.utils.data.DataLoader, model: nn.Module, phase=\"val\") -> tuple[float, float]:\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            out = model(data)\n",
    "\n",
    "            # The implementation returns only the feature vector rather than the classification logits.\n",
    "            # To compare the labels, we therefore must apply the classification layer manually:\n",
    "            out = model.classifier(out)\n",
    "\n",
    "            loss = F.cross_entropy(out, target)\n",
    "            acc = accuracy(target, out)\n",
    "\n",
    "            losses.update(loss.item(), out.shape[0])\n",
    "            accs.update(acc, out.shape[0])\n",
    "    \n",
    "    return losses.avg, accs.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf0258",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585acb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee98eb21afb4615a77f7c5cd2cdcd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Target Domain:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert MixStyle after the following layers: ['layer1', 'layer2']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce92736ab3f494a97024e1115518b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch (cartoon):   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m patience_counter = \u001b[32m0\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_domain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     train_loss, train_acc = train(epoch, target_domain, train_loader, model, optimizer)\n\u001b[32m     29\u001b[39m     test_loss, test_acc = evaluate(test_loader, model)\n\u001b[32m     31\u001b[39m     writer.add_scalar(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss/Test/target=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_domain\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, test_loss, epoch)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epoch, target_domain, data_loader, model, optimizer)\u001b[39m\n\u001b[32m     21\u001b[39m loss.backward()\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m acc = accuracy(target, out)\n\u001b[32m     25\u001b[39m losses.update(loss.item(), out.shape[\u001b[32m0\u001b[39m])\n\u001b[32m     26\u001b[39m accs.update(acc, out.shape[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36maccuracy\u001b[39m\u001b[34m(target, output)\u001b[39m\n\u001b[32m      3\u001b[39m _, pred = torch.max(output, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m      4\u001b[39m correct = pred.eq(target).sum()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m correct.item() / batch_size\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_results = {d: [] for d in DOMAINS}\n",
    "all_results['avgs'] = []\n",
    "all_results['worst'] = []\n",
    "\n",
    "for _ in tqdm(range(NUM_SEEDS), desc=\"Seeds\"):\n",
    "    results = {}\n",
    "\n",
    "    for target_domain in tqdm(DOMAINS, desc=\"Target Domain\"):\n",
    "        model = build_model()\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = build_optimizer(model)\n",
    "        scheduler = build_scheduler(optimizer)\n",
    "\n",
    "        if not USE_PRETRAINED:\n",
    "            img_mean, img_std = pacs.get_nomalization_stats(target_domain)\n",
    "            print(f\"Normalization values excluding domain {target_domain}:\\n\\tmean: {img_mean}\\n\\tstd: {img_std}\")\n",
    "            image_transform = T.Compose([\n",
    "                *AUGMENTATIONS,\n",
    "                T.Resize(256),\n",
    "                T.CenterCrop(224),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=img_mean, std=img_std)\n",
    "            ])\n",
    "        else:\n",
    "            image_transform = pretrained_image_transform\n",
    "\n",
    "        train_loader, test_loader = pacs.get_data_loaders(target_domain, train_batch_size=BATCH_SIZE, transform=image_transform, shuffle_test=True, drop_last=True)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        for epoch in tqdm(range(1, EPOCHS + 1), desc=f\"Epoch ({target_domain})\"):\n",
    "            train_loss, train_acc = train(epoch, target_domain, train_loader, model, optimizer)\n",
    "            test_loss, test_acc = evaluate(test_loader, model)\n",
    "\n",
    "            writer.add_scalar(f\"Loss/Test/target={target_domain}\", test_loss, epoch)\n",
    "            writer.add_scalar(f\"Accuracy/Test/target={target_domain}\", test_acc, epoch)\n",
    "\n",
    "            scheduler.step(test_loss)\n",
    "\n",
    "            if best_loss - test_loss < EARLY_STOPPING_DELTA and (patience_counter := patience_counter+1) > EARLY_STOPPING_PATIENCE:\n",
    "                break\n",
    "\n",
    "            if test_loss < best_loss:\n",
    "                best_acc = test_acc\n",
    "                torch.save(model.state_dict(), f\"../checkpoints/mixstyle/best_{target_domain}.pt\")\n",
    "\n",
    "        model.load_state_dict(torch.load(f\"../checkpoints/mixstyle/best_{target_domain}.pt\"))\n",
    "        _, acc = evaluate(test_loader, model, phase=\"final\")\n",
    "\n",
    "        results[target_domain].append(acc)\n",
    "\n",
    "    avg_acc = np.mean(*results.values())\n",
    "    worst_case_acc = np.min(*results.values())\n",
    "\n",
    "    for d in DOMAINS:\n",
    "        all_results[d].extend(results)\n",
    "    all_results['avgs'].append(avg_acc)\n",
    "    all_results['worst'].append(worst_case_acc)\n",
    "\n",
    "print(\"Average Accuracy:\\n\" +\n",
    "      \"{}\".format(\"\".join(f\"\\t{d}: {np.mean(all_results[d])}, std: {np.std(all_results[d])}\\n\" for d in DOMAINS)) +\n",
    "      f\"\\ttotal: {np.mean(all_results['avgs'])}, std: {np.std(all_results['avgs'])}\"\n",
    "      \"Worst-case Accuracy:\\n\" +\n",
    "      \"{}\".format(\"\".join(f\"\\t{d}: {np.min(all_results[d])}\" for d in DOMAINS)) +\n",
    "      f\"\\ttotal: {np.mean(all_results['worst'])}, std: {np.std(all_results['worst'])}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_proj_t2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
