\section{Discussion}

As described in the previous section, freezing early layers doesn't seem to affect the performance at all. This suggests that the feature extractors in the earlier layers are quite stable even without freezing across the domains, which is plausible, as basic shapes and low-level features remain consistent across different domains.

Mixstyle on the other hand is a very promising technique, which has shown to enable great performance increases. Our results demonstrate that Mixstyle is best applied after the first two layers, with the most important layer being the first one. This makes sense as basic shapes and style information are conveyed at this stage. A perturbation at this level is hence the most effective for domain generalization. Our experiments also show that a smaller $\alpha$, i.e., a more extreme perturbation, is more effective. By introducing artificial domains with heavier stylistic changes less frequently, the model sees more variety in domains during training while still having information of the original domains. This results in a better ability to generalize across domains.

Our results also show a flaw in the experimental setup, including that commonly used in the field. Using pretrained weights is problematic as certain domains essentially become irrelevant. Prime examples for that are the ``photo'' domain in PACS as well as the ``product'' and ``real world'' domains from Office-Home. In the latter case, even two domains are affected by data leakage. This suggests that results are better gathered by not using pretrained weights when just assessing the performance of domain generalization techniques. In practice, however, pretrained weights are often beneficial, which creates a tension that warrants further investigation.

The datasets used in our study also have limitations that should be acknowledged. PACS and Office-Home represent relatively controlled scenarios with clearly defined domain boundaries. Real-world domain shifts are often more subtle and continuous, and it is unclear whether more gradual style shifts affect the effectiveness of Mixstyle. Additionally, while our tSNE analysis shows some improvement in feature clustering, the differences are quite subtle and could benefit from more detailed analysis.

Overall, looking back at the results, Mixstyle is a lightweight, parameter-free and highly effective method for domain generalization. The technique shows consistent improvements across different domains, with particularly strong gains in challenging domains like sketch. However, it is important to mention that the datasets used in our study represent controlled scenarios, and future work should investigate how well these findings transfer to more naturalistic domain shifts encountered in practical applications.