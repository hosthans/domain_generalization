@article{chongLearningDomainInvariant2021,
  title = {Learning Domain Invariant and Specific Representation for Cross-Domain Person Re-Identification},
  author = {Chong, Yanwen and Peng, Chengwei and Zhang, Chen and Wang, Yujie and Feng, Wenqiang and Pan, Shaoming},
  year = {2021},
  month = aug,
  journal = {Applied Intelligence},
  volume = {51},
  number = {8},
  pages = {5219--5232},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-020-02107-2},
  url = {https://link.springer.com/10.1007/s10489-020-02107-2},
  urldate = {2025-07-27},
  abstract = {Person re-identification (re-ID) aims to match person images under different cameras with disjoint views. Although supervised re-ID has achieved great progress, unsupervised cross-domain re-ID remains a challenging work due to domain bias. In this work, we divide cross-domain re-ID task into two phases: domain-invariant features learning and domainspecific features learning. Our contributions are twofold. (i) To achieve domain-invariant features learning, a novel model called Pedestrian General Similarity (PGS) is proposed, which can eliminate two main factors that cause domain bias: image style and background. Compared with the existing re-ID models, PGS has better generalization ability. (ii) A novel pseudo label assignment method named Mutual Nearest Neighbors Pseudo Labeling (MNNPL) is proposed, which calculates pseudo labels based on the similarity between samples in the target domain, and the resulting pseudo labels are used to guide domain-specific feature learning. Extensive experiments are conducted on several large scale datasets, the results show that our method outperforms most published unsupervised cross-domain methods by a large margin.},
  langid = {english},
  file = {/Users/max/Zotero/storage/X2HPDQYK/Chong et al. - 2021 - Learning domain invariant and specific representation for cross-domain person re-identification.pdf}
}

@inproceedings{fangUnbiasedMetricLearning2013,
  title = {Unbiased {{Metric Learning}}: {{On}} the {{Utilization}} of {{Multiple Datasets}} and {{Web Images}} for {{Softening Bias}}},
  shorttitle = {Unbiased {{Metric Learning}}},
  booktitle = {2013 {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Fang, Chen and Xu, Ye and Rockmore, Daniel N.},
  year = {2013},
  month = dec,
  pages = {1657--1664},
  publisher = {IEEE},
  address = {Sydney, Australia},
  doi = {10.1109/ICCV.2013.208},
  url = {http://ieeexplore.ieee.org/document/6751316/},
  urldate = {2025-07-27},
  abstract = {Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyperparameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validation performance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.},
  isbn = {978-1-4799-2840-8},
  langid = {english},
  file = {/Users/max/Zotero/storage/RPESRKPG/Fang et al. - 2013 - Unbiased Metric Learning On the Utilization of Multiple Datasets and Web Images for Softening Bias.pdf}
}

@misc{gulrajaniSearchLostDomain2020,
  title = {In {{Search}} of {{Lost Domain Generalization}}},
  author = {Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2020},
  month = jul,
  number = {arXiv:2007.01434},
  eprint = {2007.01434},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.01434},
  url = {http://arxiv.org/abs/2007.01434},
  urldate = {2025-07-27},
  abstract = {The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions---datasets, architectures, and model selection criteria---render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DOMAINBED, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DOMAINBED and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DOMAINBED, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/max/Zotero/storage/XRJNV3UY/Gulrajani und Lopez-Paz - 2020 - In Search of Lost Domain Generalization.pdf}
}

@misc{liDeeperBroaderArtier2017,
  title = {Deeper, {{Broader}} and {{Artier Domain Generalization}}},
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
  year = {2017},
  month = oct,
  number = {arXiv:1710.03077},
  eprint = {1710.03077},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.03077},
  url = {http://arxiv.org/abs/1710.03077},
  urldate = {2025-07-27},
  abstract = {The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/max/Zotero/storage/7EGXXUQ5/Li et al. - 2017 - Deeper, Broader and Artier Domain Generalization.pdf}
}

@misc{liuDEJAVUContinual2023,
  title = {{{DEJA VU}}: {{Continual Model Generalization For Unseen Domains}}},
  shorttitle = {{{DEJA VU}}},
  author = {Liu, Chenxi and Wang, Lixu and Lyu, Lingjuan and Sun, Chen and Wang, Xiao and Zhu, Qi},
  year = {2023},
  month = mar,
  number = {arXiv:2301.10418},
  eprint = {2301.10418},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.10418},
  url = {http://arxiv.org/abs/2301.10418},
  urldate = {2025-07-27},
  abstract = {In real-world applications, deep learning models often run in non-stationary environments where the target data distribution continually shifts over time. There have been numerous domain adaptation (DA) methods in both online and offline modes to improve cross-domain adaptation ability. However, these DA methods typically only provide good performance after a long period of adaptation, and perform poorly on new domains before and during adaptation -- in what we call the ``Unfamiliar Period'', especially when domain shifts happen suddenly and significantly. On the other hand, domain generalization (DG) methods have been proposed to improve the model generalization ability on unadapted domains. However, existing DG works are ineffective for continually changing domains due to severe catastrophic forgetting of learned knowledge. To overcome these limitations of DA and DG in handling the Unfamiliar Period during continual domain shift, we propose RaTP, a framework that focuses on improving models' target domain generalization (TDG) capability, while also achieving effective target domain adaptation (TDA) capability right after training on certain domains and forgetting alleviation (FA) capability on past domains. RaTP includes a trainingfree data augmentation module to prepare data for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for TDA, and a prototype contrastive alignment algorithm to align different domains for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and DomainNet demonstrate that RaTP significantly outperforms state-of-the-art works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG and Unified DA\&DG in TDG, and achieves comparable TDA and FA capabilities.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/max/Zotero/storage/N9D5WBK4/Liu et al. - 2023 - DEJA VU Continual Model Generalization For Unseen Domains.pdf}
}

@misc{muandetDomainGeneralizationInvariant2013,
  title = {Domain {{Generalization}} via {{Invariant Feature Representation}}},
  author = {Muandet, Krikamol and Balduzzi, David and Sch{\"o}lkopf, Bernhard},
  year = {2013},
  month = jan,
  number = {arXiv:1301.2115},
  eprint = {1301.2115},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1301.2115},
  url = {http://arxiv.org/abs/1301.2115},
  urldate = {2025-07-27},
  abstract = {This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/max/Zotero/storage/FE2R5UNU/Muandet et al. - 2013 - Domain Generalization via Invariant Feature Representation.pdf}
}

@misc{schwonbergAugmentationbasedDomainGeneralization2023,
  title = {Augmentation-Based {{Domain Generalization}} for {{Semantic Segmentation}}},
  author = {Schwonberg, Manuel and Bouazati, Fadoua El and Schmidt, Nico M. and Gottschalk, Hanno},
  year = {2023},
  month = apr,
  number = {arXiv:2304.12122},
  eprint = {2304.12122},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.12122},
  url = {http://arxiv.org/abs/2304.12122},
  urldate = {2025-07-27},
  abstract = {Unsupervised Domain Adaptation (UDA) and domain generalization (DG) are two research areas that aim to tackle the lack of generalization of Deep Neural Networks (DNNs) towards unseen domains. While UDA methods have access to unlabeled target images, domain generalization does not involve any target data and only learns generalized features from a source domain. Image-style randomization or augmentation is a popular approach to improve network generalization without access to the target domain. Complex methods are often proposed that disregard the potential of simple image augmentations for out-of-domain generalization. For this reason, we systematically study the in- and out-of-domain generalization capabilities of simple, rule-based image augmentations like blur, noise, color jitter and many more. Based on a full factorial design of experiment design we provide a systematic statistical evaluation of augmentations and their interactions. Our analysis provides both, expected and unexpected, outcomes. Expected, because our experiments confirm the common scientific standard that combination of multiple different augmentations outperforms single augmentations. Unexpected, because combined augmentations perform competitive to state-of-the-art domain generalization approaches, while being significantly simpler and without training overhead. On the challenging synthetic-to-real domain shift between Synthia and Cityscapes we reach 39.5\% mIoU compared to 40.9\% mIoU of the best previous work. When additionally employing the recent vision transformer architecture DAFormer we outperform these benchmarks with a performance of 44.2\% mIoU.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/max/Zotero/storage/J4J4MCWD/Schwonberg et al. - 2023 - Augmentation-based Domain Generalization for Semantic Segmentation.pdf}
}

@inproceedings{venkateswaraDeepHashingNetwork2017,
  title = {Deep {{Hashing Network}} for {{Unsupervised Domain Adaptation}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Venkateswara, Hemanth and Eusebio, Jose and Chakraborty, Shayok and Panchanathan, Sethuraman},
  year = {2017},
  month = jul,
  pages = {5385--5394},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.572},
  url = {http://ieeexplore.ieee.org/document/8100055/},
  urldate = {2025-07-27},
  isbn = {978-1-5386-0457-1}
}

@article{wangDeepVisualDomain2018,
  title = {Deep Visual Domain Adaptation: {{A}} Survey},
  shorttitle = {Deep Visual Domain Adaptation},
  author = {Wang, Mei and Deng, Weihong},
  year = {2018},
  month = oct,
  journal = {Neurocomputing},
  volume = {312},
  pages = {135--153},
  issn = {09252312},
  doi = {10.1016/j.neucom.2018.05.083},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218306684},
  urldate = {2025-07-27},
  langid = {english},
  file = {/Users/max/Zotero/storage/NLMHWYQD/Wang und Deng - 2018 - Deep visual domain adaptation A survey.pdf}
}

@article{yangLearningDomainInvariantDiscriminative2020,
  title = {Learning {{Domain-Invariant Discriminative Features}} for {{Heterogeneous Face Recognition}}},
  author = {Yang, Shanmin and Fu, Keren and Yang, Xiao and Lin, Ye and Zhang, Jianwei and Peng, Cheng},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {209790--209801},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3038906},
  url = {https://ieeexplore.ieee.org/document/9262951/},
  urldate = {2025-07-27},
  abstract = {Heterogeneous face recognition (HFR), referring to matching face images across different domains, is a challenging problem due to the vast cross-domain discrepancy and insufficient pairwise cross-domain training data. This paper proposes a quadruplet framework for learning domain-invariant discriminative features (DIDF) for HFR, which integrates domain-level and class-level alignment in one unified network. The domain-level alignment reduces the cross-domain distribution discrepancy. The classlevel alignment based on a special quadruplet loss is developed to further diminish the intra-class variations and enlarge the inter-class separability among instances, thus handling the misalignment and adversarial equilibrium problems confronted by the domain-level alignment. With a bidirectional cross-domain data selection strategy, the quadruplet loss-based method prominently enriches the training set and further eliminates the cross-modality shift. Benefiting from the joint supervision and mutual reinforcement of these two components, the domain invariance and class discrimination of identity features are guaranteed. Extensive experiments on the challenging CASIA NIR-VIS 2.0 database, the Oulu-CASIA NIR\&VIS database, the BUAA-VisNir database, and the IIIT-D viewed sketch database demonstrate the effectiveness and preferable generalization capability of the proposed method.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/max/Zotero/storage/YREDYGFU/Yang et al. - 2020 - Learning Domain-Invariant Discriminative Features for Heterogeneous Face Recognition.pdf}
}

@misc{zhongBridgingDomainsApproximately2024,
  title = {Bridging {{Domains}} with {{Approximately Shared Features}}},
  author = {Zhong, Ziliang Samuel and Pan, Xiang and Lei, Qi},
  year = {2024},
  month = mar,
  number = {arXiv:2403.06424},
  eprint = {2403.06424},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.06424},
  url = {http://arxiv.org/abs/2403.06424},
  urldate = {2025-07-27},
  abstract = {Multi-source domain adaptation aims to reduce performance degradation when applying machine learning models to unseen domains. A fundamental challenge is devising the optimal strategy for feature selection. Existing literature is somewhat paradoxical: some advocate for learning invariant features from source domains, while others favor more diverse features. To address the challenge, we propose a statistical framework that distinguishes the utilities of features based on the variance of their correlation to label y across domains. Under our framework, we design and analyze a learning procedure consisting of learning approximately shared feature representation from source tasks and fine-tuning it on the target task. Our theoretical analysis necessitates the importance of learning approximately shared features instead of only the strictly invariant features and yields an improved population risk compared to previous results on both source and target tasks, thus partly resolving the paradox mentioned above. Inspired by our theory, we proposed a more practical way to isolate the content (invariant+approximately shared) from environmental features and further consolidate our theoretical findings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/max/Zotero/storage/X7TM2ARB/Zhong et al. - 2024 - Bridging Domains with Approximately Shared Features.pdf}
}

@misc{zhouMixStyleNeuralNetworks2023,
  title = {{{MixStyle Neural Networks}} for {{Domain Generalization}} and {{Adaptation}}},
  author = {Zhou, Kaiyang and Yang, Yongxin and Qiao, Yu and Xiang, Tao},
  year = {2023},
  month = sep,
  number = {arXiv:2107.02053},
  eprint = {2107.02053},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.02053},
  url = {http://arxiv.org/abs/2107.02053},
  urldate = {2025-07-09},
  abstract = {Neural networks do not generalize well to unseen data with domain shifts -- a longstanding problem in machine learning and AI. To overcome the problem, we propose MixStyle, a simple plug-and-play, parameter-free module that can improve domain generalization performance without the need to collect more data or increase model capacity. The design of MixStyle is simple: it mixes the feature statistics of two random instances in a single forward pass during training. The idea is grounded by the finding from recent style transfer research that feature statistics capture image style information, which essentially defines visual domains. Therefore, mixing feature statistics can be seen as an efficient way to synthesize new domains in the feature space, thus achieving data augmentation. MixStyle is easy to implement with a few lines of code, does not require modification to training objectives, and can fit a variety of learning paradigms including supervised domain generalization, semi-supervised domain generalization, and unsupervised domain adaptation. Our experiments show that MixStyle can significantly boost out-of-distribution generalization performance across a wide range of tasks including image recognition, instance retrieval and reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/max/Zotero/storage/5EL4K4B5/Zhou et al. - 2023 - MixStyle Neural Networks for Domain Generalization and Adaptation.pdf}
}
