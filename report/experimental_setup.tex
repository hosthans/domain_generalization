\section{Experimental Setup}\label{sec:setup}

Building on the provided theoretical foundation, this section offers an overview over the employed datasets, the experimental setup as well as the models and evaluation metrics used.

\subsection{Datasets}

PACS \citep{liDeeperBroaderArtier2017} is an image dataset specifically designed for domain generalization. It contains seven classes (e.g., ``dog'', ``house'', ``guitar''), distributed across four domains (photo, art painting, cartoon, sketch). These domain shifts are rather extreme, which is ideal for testing style robustness.

Office-Home \citep{venkateswaraDeepHashingNetwork2017} is another common dataset for domain adaptation and generalization. It has 65 classes containing objects commonly found in office and home environments from four domains: art, clip art, product and real-world. It hence offers a more realistic style variation than PACS and provides us with a different setting in which we can test generalization ability.

\subsection{Technical Setup}

Both datasets are available on HuggingFace and were accessed via the \lstinline|datasets| package. For easy access, the datasets were wrapped in a pandas data frame and accessed with a custom \lstinline|DataLoader|. PACS is a comparably small dataset, so it fits into memory. For Office-Home, images are lazily loaded via indexing. The \lstinline|DataLoader| API, however, hides this technicality from the user, it enables to iterate through the dataset providing $(\mathtt{image}, \mathtt{domain}, \mathtt{class})$-triples.

Our evaluation was conducted in a Jupyter notebook as it provides a suitable environment for quick changes and good overview. We evaluated in a multi-source domain setting, where we trained the model on three domains, leaving one out for validation. In a later ablation study, we only trained on two domains to have a ``real'' test set.

\begin{table}
    \centering
    \caption{Final hyperparameters}
    \begin{tabular}{l l}
        \toprule
        Epochs & $25$ \\
        Batch size & $32$ \\
        Learning rate & $0.001$ \\
        Weight decay & $0.0001$ \\
        Optimizer & SGD with momentum ($\beta = 0.9$) \\
        LR scheduler & Cosine annealing \\
        Early stopping & 10 epochs (delta: $0.00001$) \\
        \bottomrule
    \end{tabular}
    \label{tab:hyperparameters}
\end{table}

Hyperparameters were tuned manually and are listed in table \ref{tab:hyperparameters}. You may notice, that for early stopping, we have a patience of 10, but are training for at most 25 epochs. Due to time constraints, we couldn't find a suitable configuration that employs early stopping and at the same time ensures enough training.

\subsection{Models and Metrics}

In early experiments, we experimented with different CNN architectures as backbones. These experiments included VGG-16 \citep{vgg-16_ref} and the two ResNet architectures ResNet-18 and ResNet-50 \citep{resnet_ref}. VGG-16 performed far worse than the ResNet architectures, so we did not consider it further. For the ResNet-18 and ResNet-50, we loaded their weights pretrained on ImageNet. Note though, that this introduces the problem of data leakage. E.g., ImageNet consists for the most part of photos and in our evaluation setup, we would later train on every domain but photo on which we evaluate. Since we are using pretrained weights, however, the model will have seen lots of photos, which makes this domain obsolete for training.

The most common evaluation metric reported for domain generalization is accuracy. We hence collected the accuracy across domains as well as overall accuracy. The results are given as mean and standard deviation across three seeds.